{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYWiPauv02Au",
        "outputId": "9bee3c5f-4127-40e2-896c-3b2a01366d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Customized Text Generation\n",
            "Temperature: 0.7, Top-k: 10, Top-p: 0.9\n",
            "the future of ai lies in cats the they cats blue cats they sun blue blue sun bright. sky blue sun are sun love are they sun sky love sun bright. they cats bright. cats blue cats sun sun the sky blue cats they the are blue the sun bright. the sun they cats cats are\n",
            "Temperature: 1.5, Top-k: 5, Top-p: 0.95\n",
            "the future of ai lies in are love sky are love love are sun sun sky are are love love they love love sky sky love sky are sky sky sun sky sun they sun sun sky sun sun sky love sky love love they sky are sky sun are are they sky they love are\n",
            "\n",
            "Task 2: Sentiment Classification with Custom Tokens\n",
            "Review: This is an outstanding masterpiece!\n",
            "Predicted Sentiment: positive\n",
            "Scores: [0.020004788413643837, 0.012879157438874245]\n",
            "Review: I wouldnâ€™t recommend watching this movie.\n",
            "Predicted Sentiment: positive\n",
            "Scores: [0.019394857808947563, 0.015383508056402206]\n",
            "Review: Decent but forgettable performance.\n",
            "Predicted Sentiment: positive\n",
            "Scores: [0.019972890615463257, 0.013492453843355179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a1cf6c6a5841>:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  logits = model(torch.tensor(prompt_encoded[:-1]), torch.tensor([prompt_encoded[-1]], dtype=torch.long))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# Small text dataset (context)\n",
        "context = [\n",
        "    \"The sky is blue and the sun is bright.\",\n",
        "    \"Cats are independent animals, but they love attention.\"\n",
        "]\n",
        "\n",
        "# Tokenizer (simplistic)\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Vocabulary and mappings\n",
        "vocab = list(set(token for sentence in context for token in tokenize(sentence)))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "# Reinitialize embedding layer\n",
        "def reinitialize_embedding():\n",
        "    global model\n",
        "    model.embedding = nn.Embedding(len(vocab), embed_dim)\n",
        "    print(f\"Embedding layer reinitialized with vocab size: {len(vocab)}\")\n",
        "\n",
        "\n",
        "# Encode and decode functions\n",
        "def encode(text):\n",
        "    global vocab, word_to_idx, idx_to_word\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    for token in tokens:\n",
        "        if token not in word_to_idx:\n",
        "            # Add new tokens to the vocabulary\n",
        "            new_idx = len(vocab)\n",
        "            vocab.append(token)\n",
        "            word_to_idx[token] = new_idx\n",
        "            idx_to_word[new_idx] = token\n",
        "\n",
        "    return torch.tensor([word_to_idx[token] for token in tokens], dtype=torch.long)\n",
        "\n",
        "def decode(indices):\n",
        "    return \" \".join(idx_to_word[idx.item()] for idx in indices)\n",
        "\n",
        "# Attention-based QA model\n",
        "class AttentionQA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context, question):\n",
        "        # Embedding\n",
        "        context_embed = self.embedding(context)\n",
        "        question_embed = self.embedding(question)\n",
        "\n",
        "        # Compute query, key, and value\n",
        "        query = self.query_proj(question_embed.mean(dim=0, keepdim=True))\n",
        "        keys = self.key_proj(context_embed)\n",
        "        values = self.value_proj(context_embed)\n",
        "\n",
        "        # Compute attention scores and weights\n",
        "        scores = torch.matmul(query, keys.T) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute context vector as weighted sum of values\n",
        "        context_vector = torch.matmul(attention_weights, values).squeeze(0)\n",
        "\n",
        "        # Output projection\n",
        "        logits = self.output_proj(context_vector)\n",
        "        return logits\n",
        "\n",
        "# Task 1: Customized Text Generation\n",
        "def generate_custom_text(prompt, temperature, topk, topp):\n",
        "    prompt_encoded = encode(prompt)\n",
        "    generated = prompt_encoded.tolist()\n",
        "\n",
        "    for _ in range(50):  # Generate up to 50 tokens\n",
        "        logits = model(\n",
        "            torch.tensor(generated, dtype=torch.long),\n",
        "            torch.tensor([generated[-1]], dtype=torch.long)\n",
        "        )\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-k sampling\n",
        "        topk_probs, topk_indices = torch.topk(F.softmax(logits, dim=-1), k=min(topk, logits.size(-1)))\n",
        "        topk_probs /= topk_probs.sum()\n",
        "\n",
        "        # Top-p filtering\n",
        "        sorted_probs, sorted_indices = torch.sort(topk_probs, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        cutoff_index = (cumulative_probs > topp).nonzero(as_tuple=True)[0].min().item()\n",
        "        valid_indices = sorted_indices[:cutoff_index + 1]\n",
        "\n",
        "        # Choose next token\n",
        "        next_token = random.choices(valid_indices.tolist(), weights=sorted_probs[:cutoff_index + 1].tolist())[0]\n",
        "\n",
        "        # Add token to generated list\n",
        "        if next_token >= len(vocab):\n",
        "            raise ValueError(f\"Generated token index {next_token} is out of bounds for vocab size {len(vocab)}.\")\n",
        "\n",
        "        generated.append(next_token)\n",
        "\n",
        "        if idx_to_word[next_token] == \"<eos>\":  # End of sequence token\n",
        "            break\n",
        "\n",
        "    return decode(torch.tensor(generated))\n",
        "\n",
        "\n",
        "# Task 2: Sentiment Classification with Custom Tokens\n",
        "def zero_shot_sentiment(review):\n",
        "    prompt_template = \"The sentiment of the review '{review}' is {label}.\"\n",
        "    sentiments = [\"positive\", \"negative\"]\n",
        "\n",
        "    scores = []\n",
        "    for sentiment in sentiments:\n",
        "        prompt = prompt_template.format(review=review, label=sentiment)\n",
        "        prompt_encoded = encode(prompt)\n",
        "\n",
        "        logits = model(torch.tensor(prompt_encoded[:-1]), torch.tensor([prompt_encoded[-1]], dtype=torch.long))\n",
        "        sentiment_score = F.softmax(logits, dim=-1)[word_to_idx[sentiment]].item()\n",
        "        scores.append(sentiment_score)\n",
        "\n",
        "    best_sentiment = sentiments[scores.index(max(scores))]\n",
        "    return best_sentiment, scores\n",
        "\n",
        "# Initialize the model with extra space for dynamic updates\n",
        "embed_dim = 16\n",
        "vocab_buffer = 50  # Buffer for additional words\n",
        "model = AttentionQA(len(vocab) + vocab_buffer, embed_dim)\n",
        "\n",
        "# Add sentiment labels to vocabulary and reinitialize embedding\n",
        "if \"positive\" not in word_to_idx or \"negative\" not in word_to_idx:\n",
        "    if \"positive\" not in word_to_idx:\n",
        "        word_to_idx[\"positive\"] = len(vocab)\n",
        "        idx_to_word[len(vocab)] = \"positive\"\n",
        "        vocab.append(\"positive\")\n",
        "\n",
        "    if \"negative\" not in word_to_idx:\n",
        "        word_to_idx[\"negative\"] = len(vocab)\n",
        "        idx_to_word[len(vocab)] = \"negative\"\n",
        "        vocab.append(\"negative\")\n",
        "\n",
        "    # Ensure vocabulary size matches embedding\n",
        "if len(vocab) > model.embedding.num_embeddings:\n",
        "    reinitialize_embedding()\n",
        "\n",
        "\n",
        "# Testing Task 1\n",
        "configs = [(0.7, 10, 0.9), (1.5, 5, 0.95)]  # Added sample configs for temperature, top-k, top-p\n",
        "\n",
        "print(\"Task 1: Customized Text Generation\")\n",
        "for temp, topk, topp in configs:\n",
        "    generated_text = generate_custom_text(\"The future of AI lies in\", temp, topk, topp)\n",
        "    print(f\"Temperature: {temp}, Top-k: {topk}, Top-p: {topp}\")\n",
        "    print(generated_text)\n",
        "\n",
        "# Testing Task 2\n",
        "reviews = [\n",
        "    \"This is an outstanding masterpiece!\",\n",
        "    \"I wouldnâ€™t recommend watching this movie.\",\n",
        "    \"Decent but forgettable performance.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTask 2: Sentiment Classification with Custom Tokens\")\n",
        "for review in reviews:\n",
        "    sentiment, scores = zero_shot_sentiment(review)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")\n",
        "    print(f\"Scores: {scores}\")\n"
      ]
    }
  ]
}